{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path('/home/yyang/')\n",
    "pretrained_weights = 'gpt2-xl'\n",
    "pretrained_model_dir = root_dir / 'models' / 'transformers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_download_pretrained(cls, pretrained_model_dir: Path, pretrained_weights: str, **kw_args):\n",
    "    cache_dir = str(pretrained_model_dir / pretrained_weights)\n",
    "    try:\n",
    "        ret = cls.from_pretrained(cache_dir, **kw_args)\n",
    "    except:\n",
    "        ret = cls.from_pretrained(pretrained_weights, **kw_args)\n",
    "        ret.save_pretrained(cache_dir)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_gpt2.GPT2Tokenizer at 0x7fa480301550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = load_or_download_pretrained(AutoTokenizer, pretrained_model_dir, pretrained_weights)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyang/miniconda3/lib/python3.8/site-packages/transformers/modeling_auto.py:796: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ce3b47eda241c184d6cdde289b1c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=6431878936.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2-xl and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias', 'h.24.attn.masked_bias', 'h.25.attn.masked_bias', 'h.26.attn.masked_bias', 'h.27.attn.masked_bias', 'h.28.attn.masked_bias', 'h.29.attn.masked_bias', 'h.30.attn.masked_bias', 'h.31.attn.masked_bias', 'h.32.attn.masked_bias', 'h.33.attn.masked_bias', 'h.34.attn.masked_bias', 'h.35.attn.masked_bias', 'h.36.attn.masked_bias', 'h.37.attn.masked_bias', 'h.38.attn.masked_bias', 'h.39.attn.masked_bias', 'h.40.attn.masked_bias', 'h.41.attn.masked_bias', 'h.42.attn.masked_bias', 'h.43.attn.masked_bias', 'h.44.attn.masked_bias', 'h.45.attn.masked_bias', 'h.46.attn.masked_bias', 'h.47.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = load_or_download_pretrained(AutoModelWithLMHead, pretrained_model_dir, pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Chatbot:\n",
    "    tokenizer: AutoTokenizer\n",
    "    model: AutoModelWithLMHead\n",
    "    name : str\n",
    "        \n",
    "    def chat(self, question, max_len=100):\n",
    "        input_sentence = f\"\"\"Me: {question}\n",
    "        {self.name}:\"\"\"\n",
    "\n",
    "        input_ids = self.tokenizer.encode(input_sentence, return_tensors='pt')\n",
    "        sample_output = self.model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_len, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "        )\n",
    "        result = self.tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "        response = result[len(input_sentence)+1:]\n",
    "        response = re.split('\\n|\\S+:', response)[0]\n",
    "        response = response.replace('\\xa0', ' ').strip()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Chatbot(tokenizer, model, name='Joe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 3s, sys: 7.38 s, total: 9min 11s\n",
      "Wall time: 28.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Joe? Hi!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bot.chat('Who are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'God means love and tolerance.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat('What does God mean to you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm here on a date, okay? I just wanted to ask. So I know you don't like it when people talk about us like that, but I'm having trouble getting her to let me touch her. Joe is a man who has been in a relationship with Lela Alabaster since he was a little kid, and he'd love to touch her, to feel her through her clothes\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat('Why are you here?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the purpose of life?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat('What is the purpose of life?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, I don't think its lie, I think that what we think is true as scientists. In order for something to be called science, it has to make sense in scientific terms. We can use what is known about the natural world to understand the world in the future. Now, what we will also make use of is in order to make sense in this world now. We will come up with\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat('Is science a lie?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well, this is where we really have an advantage over Ebola. As you well know, we have already started a big, broad effort in our field with our very high powered field labs. We've sent several teams south east of the airport to try to find what's going on in the forests. It is getting quite out of hand. They are getting quite desperate, the soldiers and the villagers are getting\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat('When will corona virus end?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
